name: Performance Benchmarks

on:
  push:
    branches: [ main ]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    name: Performance benchmarks
    
    steps:
      - uses: actions/checkout@v6
      
      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true
          
      - uses: r-lib/actions/setup-r-dependencies@v2
        with:
          extra-packages: any::microbenchmark, any::bench
          needs: check
          
      - name: Install package
        run: R CMD INSTALL .
        
      - name: Run simulation benchmarks
        run: |
          library(chaoticds)
          
          # Create benchmarks directory if it doesn't exist
          if (!dir.exists("benchmark-results")) {
            dir.create("benchmark-results")
          }
          
          # Source benchmark scripts if they exist
          if (file.exists("benchmarks/simulation_benchmarks.R")) {
            cat("Running simulation benchmarks...\n")
            source("benchmarks/simulation_benchmarks.R")
          }
          
          # Run basic performance tests
          cat("=== Basic Performance Tests ===\n")
          
          # Test simulation performance
          sim_time <- system.time({
            sim_data <- simulate_logistic_map(10000, r = 3.8, x0 = 0.2)
          })
          cat("Logistic map (10k obs):", sim_time[3], "seconds\n")
          
          # Test extremal index performance
          threshold <- quantile(sim_data, 0.95)
          ei_time <- system.time({
            theta <- extremal_index_runs(sim_data, threshold, run_length = 3)
          })
          cat("Extremal index computation:", ei_time[3], "seconds\n")
          
          # Test cluster analysis performance
          cluster_time <- system.time({
            sizes <- cluster_sizes(sim_data, threshold, run_length = 3)
          })
          cat("Cluster analysis:", cluster_time[3], "seconds\n")
          
          # Save timing results
          timing_results <- data.frame(
            test = c("logistic_sim_10k", "extremal_index", "cluster_analysis"),
            time_seconds = c(sim_time[3], ei_time[3], cluster_time[3]),
            timestamp = Sys.time()
          )
          
          write.csv(timing_results, "benchmark-results/timing_results.csv", row.names = FALSE)
          cat("Benchmark results saved to benchmark-results/timing_results.csv\n")
        shell: Rscript {0}
        
      - name: Run memory benchmarks
        run: |
          library(chaoticds)
          
          cat("=== Memory Usage Tests ===\n")
          
          # Test memory usage with different data sizes
          sizes <- c(1000, 5000, 10000, 50000)
          memory_results <- data.frame(
            size = sizes,
            memory_mb = numeric(length(sizes))
          )
          
          for (i in seq_along(sizes)) {
            n <- sizes[i]
            gc()  # Clean up memory
            
            # Measure memory usage
            mem_before <- as.numeric(object.size(ls(envir = .GlobalEnv)))
            sim_data <- simulate_logistic_map(n, r = 3.8, x0 = 0.2)
            mem_after <- as.numeric(object.size(sim_data))
            
            memory_results$memory_mb[i] <- mem_after / 1024^2
            cat("Size:", n, "Memory:", round(memory_results$memory_mb[i], 2), "MB\n")
            
            rm(sim_data)
            gc()
          }
          
          write.csv(memory_results, "benchmark-results/memory_results.csv", row.names = FALSE)
          cat("Memory results saved to benchmark-results/memory_results.csv\n")
        shell: Rscript {0}
        
      - name: Upload benchmark results
        uses: actions/upload-artifact@v7
        with:
          name: benchmark-results
          path: benchmark-results/
          
      - name: Performance regression check
        run: |
          # Check if performance has regressed compared to baseline
          cat("=== Performance Regression Check ===\n")
          
          # Read current results
          if (file.exists("benchmark-results/timing_results.csv")) {
            current_results <- read.csv("benchmark-results/timing_results.csv")
            
            # Set performance thresholds (in seconds)
            thresholds <- list(
              logistic_sim_10k = 1.0,
              extremal_index = 0.5,
              cluster_analysis = 0.5
            )
            
            # Check each test
            regression_found <- FALSE
            for (test_name in names(thresholds)) {
              if (test_name %in% current_results$test) {
                current_time <- current_results$time_seconds[current_results$test == test_name]
                threshold <- thresholds[[test_name]]
                
                if (current_time > threshold) {
                  cat("PERFORMANCE REGRESSION:", test_name, "took", current_time, 
                      "seconds (threshold:", threshold, ")\n")
                  regression_found <- TRUE
                } else {
                  cat("PASS:", test_name, "took", current_time, "seconds (threshold:", threshold, ")\n")
                }
              }
            }
            
            if (regression_found) {
              cat("Performance regression detected!\n")
              # Uncomment next line to fail the build on regression
              # quit(status = 1)
            } else {
              cat("No performance regressions detected.\n")
            }
          }
        shell: Rscript {0}
